{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1.What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        ">- Ensemble Learning is a machine learning technique where multiple models (learners) are combined to make a single, stronger prediction.\n",
        "\n",
        "Key idea:\n",
        "Instead of relying on one model, ensemble learning aggregates predictions from several models to reduce errors, improve accuracy, and increase robustness—because different models can compensate for each other’s mistakes.\n",
        "\n",
        "Common examples: Bagging, Boosting, and Random Forest.\n",
        "\n",
        "Q2. What is the difference between Bagging and Boosting?\n",
        ">- Difference between Bagging and Boosting (short):\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "Trains multiple models independently on different random samples of the data and combines their predictions to reduce variance.\n",
        "Example: Random Forest\n",
        "\n",
        "Boosting:\n",
        "Trains models sequentially, where each new model focuses more on the previous model’s errors, aiming to reduce bias.\n",
        "Example: AdaBoost, Gradient Boosting\n",
        "\n",
        "Q3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        ">- Bootstrap sampling is a technique where multiple training datasets are created by randomly sampling with replacement from the original dataset.\n",
        "\n",
        "Role in Bagging / Random Forest:\n",
        "It allows each model (tree) to be trained on a different subset of data, increasing model diversity and reducing overfitting and variance when their predictions are combined.\n",
        "\n",
        "Q4.  What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        ">- Out-of-Bag (OOB) samples are the data points not included in a bootstrap sample for a particular model in an ensemble.\n",
        "\n",
        "OOB score:\n",
        "The ensemble model predicts these OOB samples, and the accuracy on these unseen points is used as an internal estimate of model performance without needing a separate test set.\n",
        "\n",
        "Q5.Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        ">- Feature Importance in a Single Decision Tree:\n",
        "\n",
        "Calculated based on how much a feature reduces impurity (like Gini or Entropy) in that tree.\n",
        "\n",
        "Can be unstable—small changes in data may change importance.\n",
        "\n",
        "Feature Importance in Random Forest:\n",
        "\n",
        "Averaged over all trees in the forest.\n",
        "\n",
        "More robust and reliable, as it considers many models instead of just one.\n",
        "\n",
        "Q6. Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "WhM37oPXdLQZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiE7m2d4dIYb",
        "outputId": "89bfe2b0-7bd3-4cae-b8f5-6891a62e9e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 5 features\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(feature_importance_df.head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree?\n"
      ],
      "metadata": {
        "id": "odxK2OISgyA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(), # Changed base_estimator to estimator\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "y_pred_bag = bag.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Decision Tree Accuracy: {acc_dt:.3f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {acc_bag:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsYhdHQ8hTdf",
        "outputId": "5633e640-eba4-4b1c-d35a-69103a785112"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.000\n",
            "Bagging Classifier Accuracy: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.  Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n"
      ],
      "metadata": {
        "id": "yf7DB1beh1fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 3, 5, 7]\n",
        "}\n",
        "\n",
        "# Grid Search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Evaluate final model on test set\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(f\"Final Test Accuracy: {final_accuracy:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXLH64mOhrwp",
        "outputId": "8b4f153c-1bfb-46d5-f698-74b46cb68c06"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Test Accuracy: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n"
      ],
      "metadata": {
        "id": "rdQlOEu4iNjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X = california.data\n",
        "y = california.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor\n",
        "bag_reg = BaggingRegressor(n_estimators=50, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "y_pred_bag = bag_reg.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print Mean Squared Errors\n",
        "print(f\"Bagging Regressor MSE: {mse_bag:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBKFOJHjiHA8",
        "outputId": "f8655499-f7aa-4019-cb90-17dcc36e7eeb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2579\n",
            "Random Forest Regressor MSE: 0.2565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10.You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        ">- step 1: Choose Between Bagging or Boosting\n",
        "\n",
        "Bagging: Reduces variance, useful if individual models overfit (e.g., decision trees).\n",
        "\n",
        "Boosting: Reduces bias, sequentially improves weak learners, useful if underfitting is an issue.\n",
        "\n",
        "Approach: Start with Random Forest (Bagging) for stable performance, then try XGBoost/Gradient Boosting if boosting can improve accuracy.\n",
        "\n",
        ">- Step 2: Handle Overfitting\n",
        "\n",
        "Limit tree depth (max_depth) and minimum samples per leaf (min_samples_leaf).\n",
        "\n",
        "Use regularization in boosting models (learning_rate).\n",
        "\n",
        "Apply feature selection to remove noisy features.\n",
        "\n",
        ">- Step 3: Select Base Models\n",
        "\n",
        "Decision Trees are common base models for both Bagging and Boosting.\n",
        "\n",
        "For Bagging: Multiple deep trees can reduce variance.\n",
        "\n",
        "For Boosting: Multiple shallow trees work best to reduce bias.\n",
        "\n",
        ">- Step 4: Evaluate Performance Using Cross-Validation\n",
        "\n",
        "Use k-fold cross-validation to get robust performance metrics.\n",
        "\n",
        "Evaluate with accuracy, precision, recall, F1-score, and ROC-AUC since class imbalance is likely.\n",
        "\n",
        ">- Step 5: Justify Ensemble Learning\n",
        "\n",
        "Ensemble learning combines multiple models to reduce errors from individual models.\n",
        "\n",
        "In finance, it improves loan default prediction, minimizing risk by catching subtle patterns in customer behavior and transaction history."
      ],
      "metadata": {
        "id": "JAojbRw2ippX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Simulated loan dataset (for demonstration)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
        "                           n_redundant=5, n_classes=2, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging: Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "cv_rf = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Boosting: Gradient Boosting\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "cv_gb = cross_val_score(gb, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Random Forest CV Accuracy:\", cv_rf.mean())\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
        "print(\"Gradient Boosting CV Accuracy:\", cv_gb.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6WKdvXmibJZ",
        "outputId": "ea90df28-b940-465a-d9a6-25bf5ede624e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.89\n",
            "Random Forest CV Accuracy: 0.9014285714285715\n",
            "Gradient Boosting Accuracy: 0.9066666666666666\n",
            "Gradient Boosting CV Accuracy: 0.9199999999999999\n"
          ]
        }
      ]
    }
  ]
}